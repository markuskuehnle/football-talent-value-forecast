{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a13822",
   "metadata": {},
   "source": [
    "# 00 Data Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1debf442",
   "metadata": {},
   "source": [
    "This notebook scrapes structured player statistics for Valencia CF from [FBref](https://fbref.com) across three seasons (2022–2025). It includes:\n",
    "\n",
    "* **Seasonal data scraping:** Extracts 7 core stat tables (e.g. passing, defense, possession) for each season using `pandas.read_html` from public FBref squad pages\n",
    "* **Automated filename mapping:** Dynamically names and saves each table as a CSV in `data/raw/` using season and table type\n",
    "* **Rate limit protection:** Implements a request counter and 15-minute cooldown after 10 requests to avoid getting blocked by FBref\n",
    "* **Reproducible storage:** Skips already-downloaded files to prevent unnecessary re-fetches and ensure consistent local copies\n",
    "\n",
    "> Output of this notebook is a version-controlled local dump of raw FBref tables for further inspection, cleaning, and analysis. Scraper code is commented out after use to avoid accidental API overload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85eb92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Standard library imports\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import ssl\n",
    "import time\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from urllib.request import Request, urlopen\n",
    "import urllib.parse\n",
    "import io\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Import our scraper module using relative path\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.scrapers.fbref_scraper import scrape_fbref_squad, FBrefScraper\n",
    "from src.scrapers.transfermarkt_scraper import scrape_transfermarkt_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca1435d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c00665",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fccd411",
   "metadata": {},
   "source": [
    "## FBref Data Scraper\n",
    "- Saved to CSV files in notebooks/data/raw to avoid hitting HTTP request limit\n",
    "- Will comment the code to not run it (unless needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9a090093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Current season 2024-2025\n",
    "# df_player_stats_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_standard_12\"})[0]\n",
    "# df_player_shooting_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_shooting_12\"})[0]\n",
    "# df_player_passing_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_passing_12\"})[0]\n",
    "# df_player_passing_types_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_passing_types_12\"})[0]\n",
    "# df_player_gca_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_gca_12\"})[0]\n",
    "# df_player_defense_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_defense_12\"})[0]\n",
    "# df_player_possession_2425 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/Valencia-Stats', attrs={\"id\": \"stats_possession_12\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3274094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Season 2023-2024\n",
    "# df_player_stats_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_standard_12\"})[0]\n",
    "# df_player_shooting_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_shooting_12\"})[0]\n",
    "# df_player_passing_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_passing_12\"})[0]\n",
    "# df_player_passing_types_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_passing_types_12\"})[0]\n",
    "# df_player_gca_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_gca_12\"})[0]\n",
    "# df_player_defense_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_defense_12\"})[0]\n",
    "# df_player_possession_2324 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats', attrs={\"id\": \"stats_possession_12\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b0d8ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Season 2022-2023\n",
    "# df_player_stats_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_standard_12\"})[0]\n",
    "# df_player_shooting_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_shooting_12\"})[0]\n",
    "# df_player_passing_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_passing_12\"})[0]\n",
    "# df_player_passing_types_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_passing_types_12\"})[0]\n",
    "# df_player_gca_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_gca_12\"})[0]\n",
    "# df_player_defense_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_defense_12\"})[0]\n",
    "# df_player_possession_2223 = pd.read_html('https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats', attrs={\"id\": \"stats_possession_12\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df5cbd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Save all dataframes to CSV files for future use #####\n",
    "\n",
    "# # 1 Folder →  data/raw   (create if it doesn't exist)\n",
    "\n",
    "# RAW_DIR = Path(\"..\", \"data\", \"raw\")\n",
    "# RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # 2 Find every variable in the notebook whose name starts with df_\n",
    "# frames = {\n",
    "#     name: obj\n",
    "#     for name, obj in globals().items()\n",
    "#     if name.startswith(\"df_\") and isinstance(obj, pd.DataFrame)\n",
    "# }\n",
    "\n",
    "# # 3  Save each DataFrame to CSV\n",
    "# for name, df in frames.items():\n",
    "#     filepath = RAW_DIR / f\"{name}.csv\"\n",
    "#     df.to_csv(filepath, index=False)\n",
    "#     print(f\"{filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "667bde1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_URLS = {\n",
    "#     \"2425\": \"https://fbref.com/en/squads/dcc91a7b/Valencia-Stats\",\n",
    "#     \"2324\": \"https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats\",\n",
    "#     \"2223\": \"https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats\",\n",
    "# }\n",
    "\n",
    "# TABLE_IDS = [\n",
    "#     \"stats_standard_12\",\n",
    "#     \"stats_shooting_12\",\n",
    "#     \"stats_passing_12\",\n",
    "#     \"stats_passing_types_12\",\n",
    "#     \"stats_gca_12\",\n",
    "#     \"stats_defense_12\",\n",
    "#     \"stats_possession_12\",\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "45a20f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only 10 requests per 15 minutes\n",
    "# MAX_REQUESTS = 10\n",
    "# COOLDOWN_SECONDS = 15 * 60  # 15 minutes\n",
    "\n",
    "# request_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a0f57b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def strip_suffix(table_id: str, suffix=\"_12\") -> str:\n",
    "#     return table_id[:-len(suffix)] if table_id.endswith(suffix) else table_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565bdf80",
   "metadata": {},
   "source": [
    "We added a request counter and cooldown timer to the scraper to avoid triggering FBref’s rate limits and getting blocked after multiple table fetches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f47fb32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for season, url in BASE_URLS.items():\n",
    "#     for table_id in TABLE_IDS:\n",
    "#         table_base = strip_suffix(table_id)\n",
    "#         if table_base == \"stats_standard\":\n",
    "#             fname = f\"df_player_stats_{season}.csv\"\n",
    "#         else:\n",
    "#             fname = f\"df_player_{table_base.replace('stats_', '')}_{season}.csv\"\n",
    "#         fpath = RAW_DIR / fname\n",
    "\n",
    "#         if fpath.exists():\n",
    "#             print(f\"Skipping existing file: {fname}\")\n",
    "#             continue\n",
    "\n",
    "#         if request_counter >= MAX_REQUESTS:\n",
    "#             print(f\"Request cap hit. Cooling down for {COOLDOWN_SECONDS // 60} minutes...\")\n",
    "#             time.sleep(COOLDOWN_SECONDS)\n",
    "#             request_counter = 0\n",
    "\n",
    "#         try:\n",
    "#             print(f\"Fetching: {season} | {table_id}\")\n",
    "#             df = pd.read_html(url, attrs={\"id\": table_id})[0]\n",
    "#             df.to_csv(fpath, index=False)\n",
    "#             print(f\"Saved {fpath.name}\")\n",
    "#             request_counter += 1\n",
    "#         except Exception as e:\n",
    "#             print(f\"Failed to fetch {table_id} for {season}: {e}\")\n",
    "\n",
    "#         time.sleep(random.uniform(5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37988b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"Valencia CF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86d5a6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valencia CF URLs for different seasons\n",
    "valencia_urls = {\n",
    "    \"2425\": \"https://fbref.com/en/squads/dcc91a7b/Valencia-Stats\",\n",
    "    \"2324\": \"https://fbref.com/en/squads/dcc91a7b/2023-2024/Valencia-Stats\", \n",
    "    \"2223\": \"https://fbref.com/en/squads/dcc91a7b/2022-2023/Valencia-Stats\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439fca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR_FBREF = Path(\"..\", \"data\", \"raw\", team_name, \"fbref\")\n",
    "RAW_DIR_FBREF.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scraper instance with custom settings\n",
    "scraper = FBrefScraper(\n",
    "    output_dir=RAW_DIR_FBREF,\n",
    "    max_requests=10,\n",
    "    cooldown_seconds=15 * 60,\n",
    "    delay_range=(5, 10),\n",
    "    current_season=\"2425\"\n",
    ")\n",
    "\n",
    "# Scrape all seasons\n",
    "for season, url in valencia_urls.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Scraping Valencia CF season {season}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    result = scraper.scrape_squad_stats(url, force_overwrite=False)\n",
    "    \n",
    "    if result:\n",
    "        print(f\"Successfully scraped {len(result)} tables for season {season}\")\n",
    "    else:\n",
    "        print(f\"No data scraped for season {season}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f1542",
   "metadata": {},
   "source": [
    "# Scrape Market Value Historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f6e97",
   "metadata": {},
   "source": [
    "- Encountered difficulties scraping market data from trasnfermarkt\n",
    "- Used a service called Apify to scrape (it's paid but has a good free tier)\n",
    "- Testing it below by running scraper in browser and saving file\n",
    "- Still needs adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dd671073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ── 1) read the file (local) ──────────────────────────────────────────────\n",
    "# json_path = pathlib.Path(\n",
    "#     \"..\", \"data\", \"raw\", \"dataset_transfermarkt_2025-06-15_15-34-31-954.json\"\n",
    "# )                     # <— adjust if you stored it elsewhere\n",
    "\n",
    "# with json_path.open(encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)          # top level is a list with a single club dict\n",
    "\n",
    "# # ── 2) flatten the “players” list into a table ────────────────────────────\n",
    "# club_record = data[0]            # only one element\n",
    "# players_raw = club_record[\"players\"]\n",
    "\n",
    "# df_players = pd.json_normalize(players_raw)  # one row per player\n",
    "# df_players\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e0b5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a44ee8f",
   "metadata": {},
   "source": [
    "## Transfermarkt Data Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c70e19",
   "metadata": {},
   "source": [
    "- Below is the Apify scraper code to extract valencia market value of players for season 2022,23,24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6db11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os, json, requests, pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv          # pip install python-dotenv\n",
    "\n",
    "# ── environment ───────────────────────────────────────────────────────────────\n",
    "load_dotenv()\n",
    "APIFY_TOKEN = os.getenv(\"APIFY_TOKEN\")          \n",
    "if not APIFY_TOKEN:\n",
    "    raise RuntimeError(\"Set APIFY_TOKEN first – never hard-code it in notebooks!\")\n",
    "\n",
    "ACTOR  = \"curious_coder~transfermarkt\"\n",
    "ENDPT  = (f\"https://api.apify.com/v2/acts/{ACTOR}\"\n",
    "          \"/run-sync-get-dataset-items?token=\" + APIFY_TOKEN +\n",
    "          \"&clean=true&format=json\")\n",
    "\n",
    "BASE_URL = (\"https://www.transfermarkt.co.uk/valencia-cf/kader/verein/1049/\"\n",
    "            \"plus/0/galerie/0?saison_id={year}\")\n",
    "\n",
    "def fetch_squad(year: int) -> pd.DataFrame:\n",
    "    #Run the Transfermarkt actor for one Valencia squad year → DataFrame.\n",
    "    payload = {\n",
    "        \"startUrls\": [ { \"url\": BASE_URL.format(year=year) } ],  # <- corrected\n",
    "        \"proxyConfiguration\": { \"useApifyProxy\": True },         # free pool only\n",
    "        \"maxCrawlingDepth\": 0\n",
    "    }\n",
    "\n",
    "    r = requests.post(ENDPT, json=payload, timeout=180)\n",
    "    if r.status_code >= 400:\n",
    "        raise RuntimeError(f\"{year}: HTTP {r.status_code}\\n{r.text}\")\n",
    "\n",
    "    rows = r.json()\n",
    "    if rows and \"error.type\" in rows[0]:\n",
    "        msg = rows[0].get(\"error.message\", \"no message\")\n",
    "        raise RuntimeError(f\"{year}: actor error – {msg}\")\n",
    "\n",
    "    # actor returns one club record → extract player list\n",
    "    club_record    = rows[0]\n",
    "    players_raw    = club_record[\"players\"]\n",
    "    df_players     = pd.json_normalize(players_raw)\n",
    "    df_players[\"Season\"] = year\n",
    "    return df_players\n",
    "\n",
    "\n",
    "# ── fetch three seasons & inspect ────────────────────────────────────────────\n",
    "seasons   = [2022, 2023, 2024]\n",
    "valencia_player_value  = pd.concat([fetch_squad(y) for y in seasons], ignore_index=True)\n",
    "\n",
    "valencia_player_value.to_csv(RAW_DIR / \"valencia_market_value_22_25.csv\", index=False)\n",
    "\n",
    "# valencia_player_value = pd.read_csv(RAW_DIR / \"valencia_market_value_22_25.csv\")\n",
    "\n",
    "# valencia_player_value.head()\n",
    "\n",
    "# javi_guerra_rows = valencia_player_value[valencia_player_value['Player'].astype(str).str.contains('Javi Guerra')]\n",
    "# javi_guerra_rows\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a48500",
   "metadata": {},
   "source": [
    "**Expected Format**\n",
    "\n",
    "| # | Player | Age | Current club | Market value | Nat. | Season | Contract |\n",
    "|---|--------|-----|--------------|--------------|------|--------|----------|\n",
    "| 36.0 | ['Javi Guerra', 'Central Midfield'] | 20 | Valencia CF | €2.00m | Spain | 2022 | NaN |\n",
    "| 8.0 | ['Javi Guerra', 'Central Midfield'] | 21 | Valencia CF | €20.00m | Spain | 2023 | NaN |\n",
    "| 8.0 | ['Javi Guerra', 'Central Midfield'] | 22 | NaN | €25.00m | Spain | 2024 | Jun 30, 2027 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b6aa32",
   "metadata": {},
   "source": [
    "- We can see the market value of Javi Guerra.\n",
    "- Interesting features are: Position, Market Value, Contract length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbe130",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b6708",
   "metadata": {},
   "source": [
    "# Transfermarkt Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c57e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = \"Valencia CF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "393db11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DIR_TRANSFERMARKET = Path(\"..\", \"data\", \"raw\", team_name, \"transfermarkt\")\n",
    "RAW_DIR_TRANSFERMARKET.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6403f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_season = 2020\n",
    "max_season = 2024\n",
    "\n",
    "valencia_team_data = scrape_transfermarkt_team(\n",
    "    team_name=team_name,\n",
    "    min_season=min_season,\n",
    "    max_season=max_season,\n",
    "    output_dir=RAW_DIR_TRANSFERMARKET,\n",
    "    output_filename=\"valencia_market_values_22_25.csv\",\n",
    "    drop_metadata_columns=True\n",
    ")\n",
    "\n",
    "valencia_team_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17729d2",
   "metadata": {},
   "source": [
    "NOTE: This is our raw data, which we will save before cleaning it in the next pipeline step and merging it with the other data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7a134",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42faf5d",
   "metadata": {},
   "source": [
    "# Scrape multiple teams (transfermarkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d406e43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Processing team 1/9: Real Madrid CF\n",
      "============================================================\n",
      "File already exists for Real Madrid CF, skipping...\n",
      "\n",
      "============================================================\n",
      "Processing team 2/9: FC Barcelona\n",
      "============================================================\n",
      "Waiting 53.6 seconds to avoid rate limiting...\n",
      "File already exists for FC Barcelona, skipping...\n",
      "\n",
      "============================================================\n",
      "Processing team 3/9: Atlético Madrid\n",
      "============================================================\n",
      "Waiting 68.6 seconds to avoid rate limiting...\n",
      "File already exists for Atlético Madrid, skipping...\n",
      "\n",
      "============================================================\n",
      "Processing team 4/9: Sevilla FC\n",
      "============================================================\n",
      "Waiting 79.5 seconds to avoid rate limiting...\n",
      "Starting to scrape Sevilla FC players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Sevilla FC season 2020...\n",
      "Successfully scraped 38 players for Sevilla FC season 2020\n",
      "Waiting 1.1 seconds before next request...\n",
      "Scraping Sevilla FC season 2021...\n",
      "Successfully scraped 43 players for Sevilla FC season 2021\n",
      "Waiting 2.7 seconds before next request...\n",
      "Scraping Sevilla FC season 2022...\n",
      "Successfully scraped 42 players for Sevilla FC season 2022\n",
      "Waiting 2.5 seconds before next request...\n",
      "Scraping Sevilla FC season 2023...\n",
      "Successfully scraped 49 players for Sevilla FC season 2023\n",
      "Waiting 1.8 seconds before next request...\n",
      "Scraping Sevilla FC season 2024...\n",
      "Successfully scraped 28 players for Sevilla FC season 2024\n",
      "\n",
      "Scraped 200 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Sevilla FC/transfermarkt/sevilla_market_values_2020_2024.csv\n",
      "Total records: 200\n",
      "Successfully scraped Sevilla FC: 200 records\n",
      "Saved to: ../data/raw/Sevilla FC/transfermarkt/sevilla_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing team 5/9: Athletic Club\n",
      "============================================================\n",
      "Waiting 66.5 seconds to avoid rate limiting...\n",
      "Starting to scrape Athletic Club players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Athletic Club season 2020...\n",
      "Successfully scraped 32 players for Athletic Club season 2020\n",
      "Waiting 2.9 seconds before next request...\n",
      "Scraping Athletic Club season 2021...\n",
      "Successfully scraped 31 players for Athletic Club season 2021\n",
      "Waiting 2.1 seconds before next request...\n",
      "Scraping Athletic Club season 2022...\n",
      "Successfully scraped 31 players for Athletic Club season 2022\n",
      "Waiting 2.1 seconds before next request...\n",
      "Scraping Athletic Club season 2023...\n",
      "Successfully scraped 31 players for Athletic Club season 2023\n",
      "Waiting 1.7 seconds before next request...\n",
      "Scraping Athletic Club season 2024...\n",
      "Successfully scraped 31 players for Athletic Club season 2024\n",
      "\n",
      "Scraped 156 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Athletic Club/transfermarkt/athletic_club_market_values_2020_2024.csv\n",
      "Total records: 156\n",
      "Successfully scraped Athletic Club: 156 records\n",
      "Saved to: ../data/raw/Athletic Club/transfermarkt/athletic_club_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing team 6/9: Villarreal CF\n",
      "============================================================\n",
      "Waiting 73.2 seconds to avoid rate limiting...\n",
      "Starting to scrape Villarreal CF players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Villarreal CF season 2020...\n",
      "Successfully scraped 36 players for Villarreal CF season 2020\n",
      "Waiting 1.1 seconds before next request...\n",
      "Scraping Villarreal CF season 2021...\n",
      "Error scraping Villarreal CF season 2021: HTTP Error 503: Service Unavailable\n",
      "Waiting 2.6 seconds before next request...\n",
      "Scraping Villarreal CF season 2022...\n",
      "Successfully scraped 43 players for Villarreal CF season 2022\n",
      "Waiting 1.2 seconds before next request...\n",
      "Scraping Villarreal CF season 2023...\n",
      "Error scraping Villarreal CF season 2023: HTTP Error 503: Service Unavailable\n",
      "Waiting 2.7 seconds before next request...\n",
      "Scraping Villarreal CF season 2024...\n",
      "Successfully scraped 28 players for Villarreal CF season 2024\n",
      "\n",
      "Scraped 107 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Villarreal CF/transfermarkt/villarreal_market_values_2020_2024.csv\n",
      "Total records: 107\n",
      "Successfully scraped Villarreal CF: 107 records\n",
      "Saved to: ../data/raw/Villarreal CF/transfermarkt/villarreal_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing team 7/9: Real Sociedad\n",
      "============================================================\n",
      "Waiting 86.5 seconds to avoid rate limiting...\n",
      "Starting to scrape Real Sociedad players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Real Sociedad season 2020...\n",
      "Successfully scraped 38 players for Real Sociedad season 2020\n",
      "Waiting 1.9 seconds before next request...\n",
      "Scraping Real Sociedad season 2021...\n",
      "Successfully scraped 40 players for Real Sociedad season 2021\n",
      "Waiting 1.1 seconds before next request...\n",
      "Scraping Real Sociedad season 2022...\n",
      "Successfully scraped 37 players for Real Sociedad season 2022\n",
      "Waiting 1.9 seconds before next request...\n",
      "Scraping Real Sociedad season 2023...\n",
      "Error scraping Real Sociedad season 2023: HTTP Error 503: Service Unavailable\n",
      "Waiting 1.6 seconds before next request...\n",
      "Scraping Real Sociedad season 2024...\n",
      "Error scraping Real Sociedad season 2024: HTTP Error 503: Service Unavailable\n",
      "\n",
      "Scraped 115 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Real Sociedad/transfermarkt/real_sociedad_market_values_2020_2024.csv\n",
      "Total records: 115\n",
      "Successfully scraped Real Sociedad: 115 records\n",
      "Saved to: ../data/raw/Real Sociedad/transfermarkt/real_sociedad_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing team 8/9: Real Betis\n",
      "============================================================\n",
      "Waiting 58.3 seconds to avoid rate limiting...\n",
      "Starting to scrape Real Betis players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Real Betis season 2020...\n",
      "Successfully scraped 33 players for Real Betis season 2020\n",
      "Waiting 1.4 seconds before next request...\n",
      "Scraping Real Betis season 2021...\n",
      "Successfully scraped 38 players for Real Betis season 2021\n",
      "Waiting 2.1 seconds before next request...\n",
      "Scraping Real Betis season 2022...\n",
      "Successfully scraped 38 players for Real Betis season 2022\n",
      "Waiting 3.0 seconds before next request...\n",
      "Scraping Real Betis season 2023...\n",
      "Successfully scraped 42 players for Real Betis season 2023\n",
      "Waiting 2.6 seconds before next request...\n",
      "Scraping Real Betis season 2024...\n",
      "Successfully scraped 30 players for Real Betis season 2024\n",
      "\n",
      "Scraped 181 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Real Betis/transfermarkt/real_betis_market_values_2020_2024.csv\n",
      "Total records: 181\n",
      "Successfully scraped Real Betis: 181 records\n",
      "Saved to: ../data/raw/Real Betis/transfermarkt/real_betis_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "Processing team 9/9: Valencia CF\n",
      "============================================================\n",
      "Waiting 59.9 seconds to avoid rate limiting...\n",
      "Starting to scrape Valencia CF players from season 2020 to 2024\n",
      "------------------------------------------------------------\n",
      "Scraping Valencia CF season 2020...\n",
      "Successfully scraped 35 players for Valencia CF season 2020\n",
      "Waiting 1.9 seconds before next request...\n",
      "Scraping Valencia CF season 2021...\n",
      "Successfully scraped 44 players for Valencia CF season 2021\n",
      "Waiting 1.9 seconds before next request...\n",
      "Scraping Valencia CF season 2022...\n",
      "Successfully scraped 40 players for Valencia CF season 2022\n",
      "Waiting 2.0 seconds before next request...\n",
      "Scraping Valencia CF season 2023...\n",
      "Successfully scraped 36 players for Valencia CF season 2023\n",
      "Waiting 2.7 seconds before next request...\n",
      "Scraping Valencia CF season 2024...\n",
      "Successfully scraped 26 players for Valencia CF season 2024\n",
      "\n",
      "Scraped 181 player records\n",
      "Dropped metadata columns: ['Shirt Number', 'Photo URL', 'Profile URL']\n",
      "\n",
      "Data saved to ../data/raw/Valencia CF/transfermarkt/valencia_market_values_2020_2024.csv\n",
      "Total records: 181\n",
      "Successfully scraped Valencia CF: 181 records\n",
      "Saved to: ../data/raw/Valencia CF/transfermarkt/valencia_market_values_2020_2024.csv\n",
      "\n",
      "============================================================\n",
      "SCRAPING SUMMARY\n",
      "============================================================\n",
      "Successful scrapes (9): Real Madrid CF, FC Barcelona, Atlético Madrid, Sevilla FC, Athletic Club, Villarreal CF, Real Sociedad, Real Betis, Valencia CF\n",
      "Failed scrapes (0): \n"
     ]
    }
   ],
   "source": [
    "TEAMS = [\n",
    "    \"Real Madrid CF\",\n",
    "    \"FC Barcelona\", \n",
    "    \"Atlético Madrid\",\n",
    "    \"Sevilla FC\", \n",
    "    \"Athletic Club\",\n",
    "    \"Villarreal CF\",\n",
    "    \"Real Sociedad\",\n",
    "    \"Real Betis\",\n",
    "    \"Valencia CF\",\n",
    "]\n",
    "\n",
    "min_season = 2020\n",
    "max_season = 2024\n",
    "\n",
    "# Track successful and failed scrapes\n",
    "successful_scrapes = []\n",
    "failed_scrapes = []\n",
    "\n",
    "for i, current_team_name in enumerate(TEAMS):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing team {i+1}/{len(TEAMS)}: {current_team_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Add longer delay between teams to avoid rate limiting\n",
    "    if i > 0:  # Skip delay for first team\n",
    "        delay_seconds = random.uniform(45, 90)  # 45-90 seconds between teams\n",
    "        print(f\"Waiting {delay_seconds:.1f} seconds to avoid rate limiting...\")\n",
    "        time.sleep(delay_seconds)\n",
    "    \n",
    "    team_slug = current_team_name.lower().replace(\" \", \"_\").replace(\"cf\", \"\").replace(\"fc\", \"\").strip(\"_\")\n",
    "    raw_dir_transfermarkt = Path(\"..\", \"data\", \"raw\", current_team_name, \"transfermarkt\")\n",
    "    raw_dir_transfermarkt.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    output_filename = f\"{team_slug}_market_values_{min_season}_{max_season}.csv\"\n",
    "    \n",
    "    # Check if file already exists to avoid re-scraping\n",
    "    if (raw_dir_transfermarkt / output_filename).exists():\n",
    "        print(f\"File already exists for {current_team_name}, skipping...\")\n",
    "        successful_scrapes.append(current_team_name)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        team_data = scrape_transfermarkt_team(\n",
    "            team_name=current_team_name,\n",
    "            min_season=min_season,\n",
    "            max_season=max_season,\n",
    "            output_dir=raw_dir_transfermarkt,\n",
    "            output_filename=output_filename,\n",
    "            drop_metadata_columns=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Successfully scraped {current_team_name}: {len(team_data)} records\")\n",
    "        print(f\"Saved to: {raw_dir_transfermarkt / output_filename}\")\n",
    "        successful_scrapes.append(current_team_name)\n",
    "        \n",
    "    except Exception as error:\n",
    "        print(f\"Error scraping {current_team_name}: {error}\")\n",
    "        failed_scrapes.append(current_team_name)\n",
    "        \n",
    "        # If we get a 503 error, wait longer before continuing\n",
    "        if \"503\" in str(error) or \"Service Unavailable\" in str(error):\n",
    "            print(\"Detected rate limiting (503 error). Waiting 5 minutes before continuing...\")\n",
    "            time.sleep(300)  # Wait 5 minutes\n",
    "        else:\n",
    "            print(\"Continuing with next team...\")\n",
    "        \n",
    "        continue\n",
    "\n",
    "# Summary at the end\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"SCRAPING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successful scrapes ({len(successful_scrapes)}): {', '.join(successful_scrapes)}\")\n",
    "print(f\"Failed scrapes ({len(failed_scrapes)}): {', '.join(failed_scrapes)}\")\n",
    "\n",
    "if failed_scrapes:\n",
    "    print(f\"\\nTo retry failed scrapes, run this code:\")\n",
    "    print(f\"failed_teams = {failed_scrapes}\")\n",
    "    print(\"Then run the scraping loop again with only the failed teams.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf25ca1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e137f73b",
   "metadata": {},
   "source": [
    "# Scrape multiple teams (FBRef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ea10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# 0) imports and constants\n",
    "# --------------------------------------------------------------------\n",
    "from pathlib import Path\n",
    "import random, time, requests, pandas as pd, unicodedata, io\n",
    "\n",
    "TABLE_IDS = [\n",
    "    \"stats_standard_12\", \"stats_shooting_12\", \"stats_passing_12\",\n",
    "    \"stats_passing_types_12\", \"stats_gca_12\", \"stats_defense_12\",\n",
    "    \"stats_possession_12\",\n",
    "]\n",
    "SEASONS = [\"2223\", \"2324\", \"2425\"]\n",
    "\n",
    "TEAMS = {\n",
    "    # \"Real Madrid CF\":   \"53a2f082\", # We're getting 404s for this one, so we'll skip it for now\n",
    "    \"FC Barcelona\":     \"206d90db\",\n",
    "    \"Sevilla FC\":       \"ad2be733\",\n",
    "    # \"Atlético Madrid\":  \"db3b9613\",\n",
    "    \"Athletic Club\":    \"2b390eca\",\n",
    "    \"Villarreal CF\":    \"2a8183b3\",\n",
    "    \"Real Sociedad\":    \"e31d1cd9\",\n",
    "    \"Real Betis\":       \"fc536746\",\n",
    "    \"Valencia CF\":      \"dcc91a7b\",\n",
    "}\n",
    "\n",
    "def slugify(name: str) -> str:\n",
    "    \"\"\"Convert 'Atlético Madrid' → 'Atletico-Madrid-Stats'.\"\"\"\n",
    "    base = name.replace(\" CF\", \"\").replace(\" FC\", \"\")  # optional trims\n",
    "    base = unicodedata.normalize(\"NFD\", base)          # strip accents\n",
    "    base = \"\".join(c for c in base if unicodedata.category(c) != \"Mn\")\n",
    "    return base.replace(\" \", \"-\") + \"-Stats\"\n",
    "\n",
    "def polite_get(url: str) -> requests.Response:\n",
    "    hdr = {\"User-Agent\": \"Mozilla/5.0 (polite-bot/0.1)\"}\n",
    "    time.sleep(random.uniform(5, 10))\n",
    "    return requests.get(url, headers=hdr, timeout=40)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1) main loop\n",
    "# --------------------------------------------------------------------\n",
    "for team, squad_id in TEAMS.items():\n",
    "    raw_dir = Path(\"..\", \"data\", \"raw\", team, \"fbref\")\n",
    "    raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "    slug = slugify(team)\n",
    "\n",
    "    for season in SEASONS:\n",
    "\n",
    "        # build correct base URL\n",
    "        if season == \"2425\":\n",
    "            base_url = f\"https://fbref.com/en/squads/{squad_id}/{slug}\"\n",
    "        else:\n",
    "            base_url = (\n",
    "                f\"https://fbref.com/en/squads/{squad_id}/20{season[:2]}-20{season[2:]}/{slug}\"\n",
    "            )\n",
    "\n",
    "        # quick 404 check – skip if the page isn’t live yet\n",
    "        resp = polite_get(base_url)\n",
    "        if resp.status_code == 404:\n",
    "            print(f\"{team} {season}: page not found – skipped\")\n",
    "            continue\n",
    "\n",
    "        for t_id in TABLE_IDS:\n",
    "            name_part = t_id.replace(\"stats_\", \"\").replace(\"_12\", \"\")\n",
    "            csv_name  = (\n",
    "                f\"df_player_stats_{season}.csv\"\n",
    "                if t_id == \"stats_standard_12\"\n",
    "                else f\"df_player_{name_part}_{season}.csv\"\n",
    "            )\n",
    "            out_path = raw_dir / csv_name\n",
    "            if out_path.exists():\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # pandas ≥3.0 wants StringIO, and we catch “no table” errors\n",
    "                df = pd.read_html(io.StringIO(resp.text), attrs={\"id\": t_id})[0]\n",
    "            except ValueError:\n",
    "                print(f\"{team} {season} {t_id}: table missing – skipped\")\n",
    "                continue\n",
    "\n",
    "            df.to_csv(out_path, index=False)\n",
    "            print(f\"{team} {season} {t_id} -> {out_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319bfa2",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49ce649",
   "metadata": {},
   "source": [
    "Since we're getting 404s for Real Madrid CF, we'll use Selenium to scrape the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4f36479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Dynamic Selenium Scraper that Detects Correct Table IDs\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def setup_selenium_driver() -> webdriver.Chrome:\n",
    "    \"\"\"Initialize headless Chrome driver for web scraping.\"\"\"\n",
    "    chrome_options: Options = Options()\n",
    "    chrome_options.headless = True\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def wait_for_table_to_load(selenium_driver: webdriver.Chrome, table_id: str, timeout_seconds: int = 10) -> bool:\n",
    "    \"\"\"Wait for a specific table to be present in the DOM.\"\"\"\n",
    "    try:\n",
    "        WebDriverWait(selenium_driver, timeout_seconds).until(\n",
    "            EC.presence_of_element_located((By.ID, table_id))\n",
    "        )\n",
    "        return True\n",
    "    except Exception as error:\n",
    "        print(f\"Table {table_id} not found: {error}\")\n",
    "        return False\n",
    "\n",
    "def detect_table_ids_on_page(selenium_driver: webdriver.Chrome) -> list:\n",
    "    \"\"\"Dynamically detect which table IDs are available on the current page.\"\"\"\n",
    "    # Look for any elements with 'stats' in their ID\n",
    "    all_elements_with_stats: list = selenium_driver.find_elements(By.CSS_SELECTOR, \"[id*='stats']\")\n",
    "    \n",
    "    # Extract the actual table IDs (not the wrapper divs)\n",
    "    table_ids = []\n",
    "    for element in all_elements_with_stats:\n",
    "        element_id: str = element.get_attribute(\"id\")\n",
    "        # Look for main table IDs (not the wrapper divs, links, etc.)\n",
    "        if (element_id.startswith(\"stats_\") and \n",
    "            not element_id.endswith(\"_link\") and \n",
    "            not element_id.endswith(\"_sh\") and \n",
    "            not element_id.endswith(\"_per_match_toggle\") and\n",
    "            not element_id.startswith(\"div_\") and\n",
    "            not element_id.startswith(\"tfooter_\") and\n",
    "            not element_id.startswith(\"sticky_style_\") and\n",
    "            not element_id.startswith(\"all_\")):\n",
    "            table_ids.append(element_id)\n",
    "    \n",
    "    return sorted(list(set(table_ids)))  # Remove duplicates and sort\n",
    "\n",
    "def debug_page_content(selenium_driver: webdriver.Chrome, table_id: str) -> None:\n",
    "    \"\"\"Debug function to check what tables are actually present on the page.\"\"\"\n",
    "    print(f\"\\nDebugging page content for table ID: {table_id}\")\n",
    "    \n",
    "    # Check current URL\n",
    "    current_url: str = selenium_driver.current_url\n",
    "    print(f\"Current URL: {current_url}\")\n",
    "    \n",
    "    # Check page title\n",
    "    page_title: str = selenium_driver.title\n",
    "    print(f\"Page title: {page_title}\")\n",
    "    \n",
    "    # Detect available table IDs\n",
    "    available_table_ids = detect_table_ids_on_page(selenium_driver)\n",
    "    print(f\"Available table IDs on page: {available_table_ids}\")\n",
    "    \n",
    "    # Check if the specific table ID exists\n",
    "    specific_table: list = selenium_driver.find_elements(By.ID, table_id)\n",
    "    if specific_table:\n",
    "        print(f\"Table {table_id} found in DOM\")\n",
    "    else:\n",
    "        print(f\"Table {table_id} NOT found in DOM\")\n",
    "\n",
    "def scrape_team_tables_with_selenium_dynamic(team_name: str, squad_id: str, season: str) -> None:\n",
    "    \"\"\"Scrape all statistical tables for a specific team and season using Selenium with dynamic table ID detection.\"\"\"\n",
    "    selenium_driver: webdriver.Chrome = setup_selenium_driver()\n",
    "    team_slug: str = slugify(team_name)\n",
    "    \n",
    "    # Build correct base URL for the season\n",
    "    if season == \"2425\":\n",
    "        base_url: str = f\"https://fbref.com/en/squads/{squad_id}/2024-2025/{team_slug}\"\n",
    "    else:\n",
    "        base_url: str = (\n",
    "            f\"https://fbref.com/en/squads/{squad_id}/20{season[:2]}-20{season[2:]}/{team_slug}\"\n",
    "        )\n",
    "    \n",
    "    print(f\"Navigating to: {base_url}\")\n",
    "    \n",
    "    # Navigate to page and wait for initial load\n",
    "    selenium_driver.get(base_url)\n",
    "    time.sleep(5)  # Increased initial page load wait\n",
    "    \n",
    "    # Create output directory\n",
    "    raw_directory: Path = Path(\"..\", \"data\", \"raw\", team_name, \"fbref\")\n",
    "    raw_directory.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Dynamically detect available table IDs on the page\n",
    "    available_table_ids = detect_table_ids_on_page(selenium_driver)\n",
    "    print(f\"Detected {len(available_table_ids)} available tables: {available_table_ids}\")\n",
    "    \n",
    "    # Define the table types we want to scrape\n",
    "    desired_table_types = [\n",
    "        \"stats_standard\", \"stats_shooting\", \"stats_passing\",\n",
    "        \"stats_passing_types\", \"stats_gca\", \"stats_defense\",\n",
    "        \"stats_possession\"\n",
    "    ]\n",
    "    \n",
    "    # Find matching table IDs for each desired type\n",
    "    table_ids_to_scrape = []\n",
    "    for desired_type in desired_table_types:\n",
    "        matching_ids = [tid for tid in available_table_ids if tid.startswith(desired_type)]\n",
    "        if matching_ids:\n",
    "            table_ids_to_scrape.append(matching_ids[0])  # Take the first match\n",
    "            print(f\"Found {desired_type}: {matching_ids[0]}\")\n",
    "        else:\n",
    "            print(f\"Missing {desired_type}\")\n",
    "    \n",
    "    # Scrape each detected table\n",
    "    for table_id in table_ids_to_scrape:\n",
    "        # Extract the base name for filename generation\n",
    "        table_name_part: str = table_id.replace(\"stats_\", \"\")\n",
    "        # Remove the suffix (could be _12, _719, etc.)\n",
    "        if \"_\" in table_name_part:\n",
    "            table_name_part = table_name_part.rsplit(\"_\", 1)[0]\n",
    "            \n",
    "        csv_filename: str = (\n",
    "            f\"df_player_stats_{season}.csv\"\n",
    "            if \"standard\" in table_id\n",
    "            else f\"df_player_{table_name_part}_{season}.csv\"\n",
    "        )\n",
    "            \n",
    "        output_path: Path = raw_directory / csv_filename\n",
    "        \n",
    "        if output_path.exists():\n",
    "            print(f\"{team_name} {season} {table_id}: file already exists – skipped\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing table: {table_id}\")\n",
    "        \n",
    "        # Wait for specific table to load\n",
    "        table_loaded: bool = wait_for_table_to_load(selenium_driver, table_id)\n",
    "        if not table_loaded:\n",
    "            print(f\"{team_name} {season} {table_id}: table not found in DOM – skipped\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get fresh page source after table loads\n",
    "            current_page_html: str = selenium_driver.page_source\n",
    "            table_dataframe: pd.DataFrame = pd.read_html(\n",
    "                io.StringIO(current_page_html), \n",
    "                attrs={\"id\": table_id}\n",
    "            )[0]\n",
    "            table_dataframe.to_csv(output_path, index=False)\n",
    "            print(f\"{team_name} {season} {table_id} -> {output_path.name}\")\n",
    "        except ValueError as error:\n",
    "            print(f\"{team_name} {season} {table_id}: table extraction failed – {error}\")\n",
    "            continue\n",
    "    \n",
    "    selenium_driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ac761b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to: https://fbref.com/en/squads/53a2f082/2024-2025/Real-Madrid-Stats\n",
      "Detected 11 available tables: ['stats_defense_12', 'stats_gca_12', 'stats_keeper_12', 'stats_keeper_adv_12', 'stats_misc_12', 'stats_passing_12', 'stats_passing_types_12', 'stats_playing_time_12', 'stats_possession_12', 'stats_shooting_12', 'stats_standard_12']\n",
      "Found stats_standard: stats_standard_12\n",
      "Found stats_shooting: stats_shooting_12\n",
      "Found stats_passing: stats_passing_12\n",
      "Found stats_passing_types: stats_passing_types_12\n",
      "Found stats_gca: stats_gca_12\n",
      "Found stats_defense: stats_defense_12\n",
      "Found stats_possession: stats_possession_12\n",
      "Real Madrid CF 2425 stats_standard_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_shooting_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_passing_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_passing_types_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_gca_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_defense_12: file already exists – skipped\n",
      "Real Madrid CF 2425 stats_possession_12: file already exists – skipped\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed scraper for Real Madrid 2024-25 season\n",
    "real_madrid_squad_id: str = \"53a2f082\"\n",
    "scrape_team_tables_with_selenium_dynamic(\"Real Madrid CF\", real_madrid_squad_id, \"2425\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da6f5492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to: https://fbref.com/en/squads/db3b9613/2024-2025/Atletico-Madrid-Stats\n",
      "Detected 11 available tables: ['stats_defense_12', 'stats_gca_12', 'stats_keeper_12', 'stats_keeper_adv_12', 'stats_misc_12', 'stats_passing_12', 'stats_passing_types_12', 'stats_playing_time_12', 'stats_possession_12', 'stats_shooting_12', 'stats_standard_12']\n",
      "Found stats_standard: stats_standard_12\n",
      "Found stats_shooting: stats_shooting_12\n",
      "Found stats_passing: stats_passing_12\n",
      "Found stats_passing_types: stats_passing_types_12\n",
      "Found stats_gca: stats_gca_12\n",
      "Found stats_defense: stats_defense_12\n",
      "Found stats_possession: stats_possession_12\n",
      "Atlético Madrid 2425 stats_standard_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_shooting_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_passing_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_passing_types_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_gca_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_defense_12: file already exists – skipped\n",
      "Atlético Madrid 2425 stats_possession_12: file already exists – skipped\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed scraper for Atlético Madrid 2024-25 season ()\n",
    "atletico_madrid_squad_id: str = \"db3b9613\"\n",
    "scrape_team_tables_with_selenium_dynamic(\"Atlético Madrid\", atletico_madrid_squad_id, \"2425\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70df163d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".football-talent-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
